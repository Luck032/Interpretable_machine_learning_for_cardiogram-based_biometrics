{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import shap\n",
    "import sys\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import pearsonr, spearmanr, shapiro, ttest_ind, mannwhitneyu\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from deap import creator, base, tools, algorithms\n",
    "\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from cliffs_delta import cliffs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = ...\n",
    "df = pd.read_csv(path,sep=',')\n",
    "df.drop(['CC'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['examinee'], axis=1)\n",
    "cols = data.columns\n",
    "n = len(cols)\n",
    "pearson_corr_mat = pd.DataFrame(np.eye(n), index=cols, columns=cols)\n",
    "pearson_pval_mat = pd.DataFrame(np.zeros((n, n)), index=cols, columns=cols)\n",
    "\n",
    "for i, xi in enumerate(cols):\n",
    "    for j, xj in enumerate(cols):\n",
    "        if i <= j:\n",
    "            r, p = pearsonr(data[xi], data[xj])\n",
    "            pearson_corr_mat.iat[i, j] = pearson_corr_mat.iat[j, i] = r\n",
    "            pearson_pval_mat.iat[i, j] = pearson_pval_mat.iat[j, i] = p\n",
    "\n",
    "annot = pearson_pval_mat.applymap(lambda p: '*' if p < 0.05 else '')\n",
    "\n",
    "mask = np.triu(np.ones_like(pearson_corr_mat, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(\n",
    "    pearson_corr_mat,\n",
    "    mask=mask,\n",
    "    annot=annot,\n",
    "    fmt='',\n",
    "    cmap='cividis',\n",
    "    vmin=-1, vmax=1,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(cols, rotation=90)\n",
    "ax.set_yticklabels(cols, rotation=0)\n",
    "\n",
    "plt.title(\n",
    "\"\"\"Pearson's r coefficient\"\"\",\n",
    "    fontsize=18\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.tril(np.ones(pearson_corr_mat.shape), k=-1).astype(bool)\n",
    "\n",
    "high_corr_pairs = (np.abs(pearson_corr_mat.values)[mask] > 0.9).sum()\n",
    "\n",
    "print(\"Number of pairs with |corr| > 0.9:\", high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['examinee'], axis=1)\n",
    "cols = data.columns\n",
    "n = len(cols)\n",
    "\n",
    "spearman_rho_mat  = pd.DataFrame(np.eye(n), index=cols, columns=cols)\n",
    "spearman_pval_mat = pd.DataFrame(np.zeros((n, n)), index=cols, columns=cols)\n",
    "\n",
    "for i, xi in enumerate(cols):\n",
    "    for j, xj in enumerate(cols):\n",
    "        if i <= j:\n",
    "            rho, p = spearmanr(data[xi], data[xj])\n",
    "            spearman_rho_mat.iat[i, j]  = spearman_rho_mat.iat[j, i]  = rho\n",
    "            spearman_pval_mat.iat[i, j] = spearman_pval_mat.iat[j, i] = p\n",
    "\n",
    "annot = spearman_pval_mat.applymap(lambda p: '*' if p < 0.05 else '')\n",
    "\n",
    "mask = np.triu(np.ones_like(spearman_rho_mat, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(\n",
    "    spearman_rho_mat,\n",
    "    mask=mask,\n",
    "    annot=annot,\n",
    "    fmt='',\n",
    "    cmap='cividis',\n",
    "    vmin=-1, vmax=1,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(cols, rotation=90)\n",
    "ax.set_yticklabels(cols, rotation=0)\n",
    "\n",
    "plt.title(\n",
    "\"Spearman's $\\\\rho$ coefficient\\n\",\n",
    "    fontsize=18\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.tril(np.ones(spearman_rho_mat.shape), k=-1).astype(bool)\n",
    "\n",
    "high_corr_pairs = (np.abs(spearman_rho_mat.values)[mask] > 0.9).sum()\n",
    "\n",
    "print(\"Number of pairs with |corr| > 0.9:\", high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pearson_corr_mat.columns\n",
    "\n",
    "# Set diagonals\n",
    "for mat in [pearson_corr_mat, spearman_rho_mat]:\n",
    "    np.fill_diagonal(mat.values, 1.0)\n",
    "for mat in [pearson_pval_mat, spearman_pval_mat]:\n",
    "    np.fill_diagonal(mat.values, 0.0)\n",
    "\n",
    "threshold = 0.7\n",
    "pairs = []\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        p_corr = pearson_corr_mat.iloc[i, j]\n",
    "        s_corr = spearman_rho_mat.iloc[i, j]\n",
    "        p_pval = pearson_pval_mat.iloc[i, j]\n",
    "        s_pval = spearman_pval_mat.iloc[i, j]\n",
    "\n",
    "        p_flag = abs(p_corr) > threshold\n",
    "        s_flag = abs(s_corr) > threshold\n",
    "\n",
    "        if p_flag or s_flag:\n",
    "            if p_flag and s_flag:\n",
    "                method = \"Both\"\n",
    "            elif p_flag:\n",
    "                method = \"Pearson only\"\n",
    "            else:\n",
    "                method = \"Spearman only\"\n",
    "\n",
    "            pairs.append({\n",
    "                \"Feature 1\": features[i],\n",
    "                \"Feature 2\": features[j],\n",
    "                \"Pearson\": round(p_corr, 3),\n",
    "                \"Spearman\": round(s_corr, 3),\n",
    "                \"Pearson pval\": round(p_pval, 3),\n",
    "                \"Spearman pval\": round(s_pval, 3),\n",
    "                \"Method\": method\n",
    "            })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "pairs_df = pairs_df.sort_values(by=\"Pearson\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = df[df['examinee'].str.endswith('- 1')].copy()\n",
    "df_anger = df[df['examinee'].str.endswith('- 2')].copy()\n",
    "\n",
    "df_baseline['examinee'] = df_baseline['examinee'].str.replace(' - 1', '', regex=False)\n",
    "df_anger['examinee'] = df_anger['examinee'].str.replace(' - 2', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['examinee'] = df['examinee'].str.replace(' - 1', '', regex=False)\n",
    "df['examinee'] = df['examinee'].str.replace(' - 2', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('examinee',axis=1)\n",
    "# y = df['examinee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['examinee'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subject_ID.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "    \n",
    "y = le.transform(df['examinee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "feature_names = X_train.columns\n",
    "gini_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "rf_top_10 = set(gini_importance_df.sort_values(by='Importance',ascending=False).reset_index(drop=True)['Feature'].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'StdDev': perm_importance.importances_std\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False, ignore_index=True)\n",
    "perm_top_10 = set(perm_importance_df['Feature'].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "explainer = shap.Explainer(rf)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "vals = shap_values.values\n",
    "\n",
    "if vals.ndim == 3:\n",
    "    vals = vals.mean(axis=2)\n",
    "\n",
    "importance = np.abs(vals).mean(axis=0)\n",
    "\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False, ignore_index=True)\n",
    "\n",
    "shap_top_10 = shap_importance_df['Feature'].iloc[:10].tolist()\n",
    "print(shap_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = shap_values.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sorted_features = shap_importance_df['Feature']\n",
    "sorted_importance = shap_importance_df['Importance']\n",
    "\n",
    "colors = ['#08306b' if feat in shap_top_10 else 'gray' for feat in sorted_features]\n",
    "\n",
    "bars = plt.barh(sorted_features, sorted_importance, color=colors)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xlabel(\"Mean |SHAP value|\", fontsize=14)\n",
    "plt.ylabel(\"Features\", fontsize=14)\n",
    "plt.title(\"SHAP feature importance\", fontsize=21)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#08306b', label='Top 10 Features'),\n",
    "    Patch(facecolor='gray', label='Remaining Features')\n",
    "]\n",
    "plt.legend(handles=legend_elements, fontsize=12, loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_three_intersection = rf_top_10.intersection(shap_top_10).intersection(perm_top_10)\n",
    "print(all_three_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sets\n",
    "set1 = set(rf_top_10)\n",
    "set2 = set(perm_top_10)\n",
    "set3 = set(shap_top_10)\n",
    "\n",
    "inter_all = set1 & set2 & set3\n",
    "print(\"Intersection of all three:\", inter_all)\n",
    "\n",
    "# Intersection of each two (excluding all-three)\n",
    "inter_12 = (set1 & set2) - inter_all\n",
    "inter_13 = (set1 & set3) - inter_all\n",
    "inter_23 = (set2 & set3) - inter_all\n",
    "print(\"Intersection set1 & set2 only:\", inter_12)\n",
    "print(\"Intersection set1 & set3 only:\", inter_13)\n",
    "print(\"Intersection set2 & set3 only:\", inter_23)\n",
    "\n",
    "# Unique elements of each set\n",
    "unique1 = set1 - (set2 | set3)\n",
    "unique2 = set2 - (set1 | set3)\n",
    "unique3 = set3 - (set1 | set2)\n",
    "print(\"Unique in set1:\", unique1)\n",
    "print(\"Unique in set2:\", unique2)\n",
    "print(\"Unique in set3:\", unique3)\n",
    "\n",
    "left_by_all = set(X.columns) - (set1 | set2 | set3)\n",
    "print(\"Left out by all three:\", left_by_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_three_intersection = X[list(all_three_intersection)]\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_three_intersection, y, test_size=0.33, stratify=y, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for feature in gini_importance_df['Feature']:\n",
    "    if feature in rf_top_10:\n",
    "        colors.append('#00224E')\n",
    "\n",
    "    else:\n",
    "        colors.append('gray') \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(\n",
    "    gini_importance_df['Feature'], \n",
    "    gini_importance_df['Importance'], \n",
    "    align='center', \n",
    "    color=colors,\n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Importance score', fontsize = 14)\n",
    "plt.ylabel('Feature name', fontsize = 14)\n",
    "plt.title('Gini importance', fontsize = 21)\n",
    "\n",
    "for label in plt.gca().get_yticklabels():\n",
    "    label.set_fontsize(14)\n",
    "    \n",
    "legend_elements = [\n",
    "    Patch(facecolor='#00224E', label='Top 10 features'),\n",
    "    Patch(facecolor='gray', label='Remaining features')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize = 12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for feature in perm_importance_df['Feature']:\n",
    "    if feature in perm_top_10:\n",
    "        colors.append('#00224E')\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(\n",
    "    perm_importance_df['Feature'], \n",
    "    perm_importance_df['Importance'], \n",
    "    xerr=perm_importance_df['StdDev'], \n",
    "    align='center', \n",
    "    color=colors,\n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "for label in plt.gca().get_yticklabels():\n",
    "    label.set_fontsize(14)\n",
    "    \n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Mean reduction in accuracy', fontsize = 14)\n",
    "plt.ylabel('Feature name', fontsize = 14)\n",
    "plt.title('Permutation importance', fontsize = 21)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#00224E', label='Top 10 features'),\n",
    "    Patch(facecolor='gray', label='Remaining features')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize = 12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rfecv = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "rfe_feature_set = X_train.columns[rfecv.support_]\n",
    "print(\"Optimal number of features:\", rfecv.n_features_)\n",
    "print(\"Selected features:\", list(rfe_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_train.columns.to_list()) - set(rfe_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_selected = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_selected.fit(X_train[rfe_feature_set], y_train)\n",
    "\n",
    "y_pred = rf_selected.predict(X_test[rfe_feature_set])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature-selection process below uses a Genetic Algorithm (GA) implementation\n",
    "# adapted for this project. The GA functions used here are based on the publicly\n",
    "# available implementation from:\n",
    "#     https://github.com/Luck032/GeneticAlgorithmForFeatureSelection\n",
    "#\n",
    "# The structure of the algorithm, including the evaluation strategy and \n",
    "# representation of individuals, is also largely inspired by:\n",
    "#     https://github.com/renatoosousa/GeneticAlgorithmForFeatureSelection\n",
    "#\n",
    "# Both repositories provide general-purpose frameworks for GA-based feature selection.\n",
    "# The code here follows their logic but has been adapted to fit the requirements of\n",
    "# this application (e.g., defined CV strategy, model choice, and data handling).\n",
    "#\n",
    "# Users are free to adopt, modify, or extend these components as appropriate for\n",
    "# their own machine learning workflows.\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "individual = [1 for i in range(len(X_train.columns))]\n",
    "print(\"Accuracy with all features: \\t\" +\n",
    "      str(getFitness(individual, X_train, y_train)) + \"\\n\")\n",
    "\n",
    "n_gen = 50\n",
    "n_pop = 50\n",
    "\n",
    "hof = geneticAlgorithm(X_train, y_train, n_pop, n_gen)\n",
    "\n",
    "accuracy, individual, header = bestIndividual(hof, X_train, y_train)\n",
    "print('Number of Features in Subset: \\t' + str(individual.count(1)))\n",
    "print('Feature Subset\\t: ' + str(header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_feature_set = set(['QRS_int', 'T_int', 'QT_int', 'RS_amp', 'RQ_amp', 'RT_amp', 'TT1_amp', 'TT2_amp', 'QR_slope', 'RS_slope', 'ECGQRScrest', 'ECGTcrest', 'CX_amp', 'CB_slope', 'BT_int', 'TX_int', 'RR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)\n",
    "\n",
    "X_train_selected = X_train[list(ga_feature_set)]\n",
    "X_test_selected = X_test[list(ga_feature_set)]\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred, average='macro') *100\n",
    "recall = recall_score(y_test, y_pred, average='macro') *100\n",
    "f1 = f1_score(y_test, y_pred, average='macro') *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_feature_set = set(ga_feature_set)\n",
    "rfe_feature_set = set(rfe_feature_set)\n",
    "\n",
    "print(len(set.intersection(rfe_feature_set, ga_feature_set)))\n",
    "print(set.intersection(rfe_feature_set, ga_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_feature_set - set.intersection(rfe_feature_set, ga_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_feature_set - set.intersection(rfe_feature_set, ga_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X.columns) - rfe_feature_set.union(ga_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set.intersection(rfe_feature_set, ga_feature_set).intersection(all_three_intersection)))\n",
    "print(set.intersection(rfe_feature_set, ga_feature_set).intersection(all_three_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = set(df.drop('examinee',axis=1).columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = list(set.intersection(rfe_feature_set, ga_feature_set))\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train[selected], y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test[selected])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) *100\n",
    "precision = precision_score(y_test, y_pred, average='macro') *100\n",
    "recall = recall_score(y_test, y_pred, average='macro') *100\n",
    "f1 = f1_score(y_test, y_pred, average='macro') *100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribtions of clusters of highly correlated features to biometric identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_all_subject = df.drop('examinee',axis=1).corr(method='pearson')\n",
    "\n",
    "upper_triangle = correlation_matrix_all_subject.where(np.triu(np.ones(correlation_matrix_all_subject.shape), k=1).astype(bool))\n",
    "\n",
    "threshold = 0.7\n",
    "high_corr_pairs = upper_triangle.stack().reset_index()\n",
    "high_corr_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs['Correlation'].abs() > threshold]\n",
    "\n",
    "high_corr_pairs = high_corr_pairs.sort_values(by = 'Correlation', ascending=False).reset_index(drop=True)\n",
    "\n",
    "high_corr_pairs['Correlation'] = high_corr_pairs['Correlation'].round(2)\n",
    "\n",
    "print(high_corr_pairs.sort_values(by = 'Correlation', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([high_corr_pairs['Variable 1'], high_corr_pairs['Variable 2']]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = X.columns\n",
    "\n",
    "adjacency = {}\n",
    "for _, row in high_corr_pairs.iterrows():\n",
    "    if row[\"Variable 1\"] not in adjacency:\n",
    "        adjacency[row[\"Variable 1\"]] = []\n",
    "    if row[\"Variable 2\"] not in adjacency:\n",
    "        adjacency[row[\"Variable 2\"]] = []\n",
    "    adjacency[row[\"Variable 1\"]].append(row[\"Variable 2\"])\n",
    "    adjacency[row[\"Variable 2\"]].append(row[\"Variable 1\"])\n",
    "\n",
    "def dfs(node, cluster_id, visited, clusters):\n",
    "    visited.add(node)\n",
    "    clusters[node] = cluster_id\n",
    "    for neighbor in adjacency.get(node, []):\n",
    "        if neighbor not in visited:\n",
    "            dfs(neighbor, cluster_id, visited, clusters)\n",
    "\n",
    "visited = set()\n",
    "clusters = {}\n",
    "cluster_id = 0\n",
    "for variable in adjacency.keys():\n",
    "    if variable not in visited:\n",
    "        cluster_id += 1\n",
    "        dfs(variable, cluster_id, visited, clusters)\n",
    "\n",
    "for var in all_variables:\n",
    "    if var not in clusters:\n",
    "        cluster_id += 1\n",
    "        clusters[var] = cluster_id\n",
    "\n",
    "cluster_result_df = pd.DataFrame(list(clusters.items()), columns=[\"Var\", \"Cluster\"])\n",
    "cluster_result_df.sort_values(by=\"Cluster\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_df[cluster_result_df['Cluster'] == 6]['Var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_feature = 'SB_int'\n",
    "feature_pairs = high_corr_pairs[\n",
    "    (high_corr_pairs['Variable 1'] == extract_feature) |\n",
    "    (high_corr_pairs['Variable 2'] == extract_feature)\n",
    "]\n",
    "\n",
    "high_corr_list = pd.concat([feature_pairs['Variable 1'], feature_pairs['Variable 2']])\n",
    "\n",
    "high_corr_list = high_corr_list[high_corr_list != extract_feature].reset_index(drop=True)\n",
    "\n",
    "print(feature_pairs.head())\n",
    "print(high_corr_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "original_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "\n",
    "X_train_permuted = X_train.copy()\n",
    "feature_perm = extract_feature\n",
    "X_train_permuted[feature_perm] = np.random.permutation(X_train_permuted[feature_perm])\n",
    "\n",
    "rf_permuted = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_permuted.fit(X_train_permuted, y_train)\n",
    "permuted_importances = pd.Series(rf_permuted.feature_importances_, index=X.columns)\n",
    "\n",
    "\n",
    "importance_comparison = pd.DataFrame({\n",
    "    \"Original Importance\": original_importances,\n",
    "    \"Permuted Importance\": permuted_importances,\n",
    "    \"Difference\": 100 * (permuted_importances - original_importances) / original_importances\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cluster = pd.concat((high_corr_list,pd.Series(extract_feature))).reset_index(drop=True)\n",
    "print('Shuffeld features:',perm_cluster.to_list())\n",
    "\n",
    "original_test_accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "X_test_permuted = X_test.copy()\n",
    "rng = np.random.default_rng(42)                  \n",
    "perm_idx = rng.permutation(len(X_test_permuted))           \n",
    "X_test_permuted[perm_cluster] = X_test_permuted[perm_cluster].to_numpy()[perm_idx, :]\n",
    "\n",
    "permuted_test_accuracy = accuracy_score(y_test, rf.predict(X_test_permuted))\n",
    "\n",
    "difference_in_accuracy = (permuted_test_accuracy - original_test_accuracy) / original_test_accuracy * 100\n",
    "\n",
    "print(\"Original Test Set Accuracy:\", original_test_accuracy * 100)\n",
    "print(\"Permuted Test Set Accuracy:\", permuted_test_accuracy * 100)\n",
    "print(\"Difference in Accuracy:\", difference_in_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_comparison.loc[high_corr_list, 'Difference'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_list = pd.concat((high_corr_list,pd.Series(extract_feature))).reset_index(drop=True)\n",
    "high_corr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_mask = ~importance_comparison.index.isin(high_corr_list)\n",
    "\n",
    "importance_comparison.loc[boolean_mask, 'Difference'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of emotions on cardiogram-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(c0, c1):\n",
    "    d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.20:\n",
    "        interpretation = \"Negligible effect\"\n",
    "    elif abs_d < 0.50:\n",
    "        interpretation = \"Small effect\"\n",
    "    elif abs_d < 0.80:\n",
    "        interpretation = \"Medium effect\"\n",
    "    else:\n",
    "        interpretation = \"Large effect\"\n",
    "    \n",
    "    return d, interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_baseline.drop('examinee', axis=1).columns\n",
    "results = []\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for feature in features:\n",
    "    data_baseline = df_baseline[feature].dropna()\n",
    "    data_anger = df_anger[feature].dropna()\n",
    "\n",
    "    _, p_base = shapiro(data_baseline)\n",
    "    _, p_anger = shapiro(data_anger)\n",
    "\n",
    "    normal_base = p_base > alpha\n",
    "    normal_anger = p_anger > alpha\n",
    "\n",
    "    if normal_base and normal_anger:\n",
    "        # t-test + Cohen's d\n",
    "        stat, p_val = ttest_ind(data_baseline, data_anger, equal_var=False)\n",
    "        d, interp = cohen_d(data_baseline, data_anger)\n",
    "        eff, eff_type, eff_interp = d, \"Cohen's d\", interp\n",
    "        test_used = \"t-test (Welch)\"\n",
    "    else:\n",
    "        # Mann-Whitney + Cliff's delta\n",
    "        stat, p_val = mannwhitneyu(data_baseline, data_anger, alternative='two-sided')\n",
    "        delta, size = cliffs_delta(data_baseline, data_anger)\n",
    "        eff, eff_type, eff_interp = delta, \"Cliff's delta\", size\n",
    "        test_used = \"Mann-Whitney U\"\n",
    "\n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'Test': test_used,\n",
    "        'p_value': p_val.round(3),\n",
    "        'Effect': eff,\n",
    "        'Effect Type': eff_type,\n",
    "        'Effect Interpretation': eff_interp\n",
    "    })\n",
    "\n",
    "# DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Bonferroni correction\n",
    "reject, p_adj, _, _ = multipletests(results_df['p_value'], method='bonferroni', alpha=alpha)\n",
    "results_df['p_adj'] = p_adj.round(3)\n",
    "results_df['Significant'] = reject\n",
    "\n",
    "# Split views\n",
    "significant_features = results_df[results_df['Significant'] == True]\n",
    "nonsignificant_features = results_df[results_df['Significant'] == False]\n",
    "\n",
    "print(\"\\nSignificant features:\\n\", significant_features)\n",
    "print(\"\\nNonsignificant features:\\n\", nonsignificant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsignificant_features['Feature'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsignificant_features['Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite_significance_pairs = []\n",
    "\n",
    "for _, row in high_corr_pairs.iterrows():\n",
    "    var1 = row['Variable 1']\n",
    "    var2 = row['Variable 2']\n",
    "    \n",
    "    var1_row = results_df.loc[results_df['Feature'] == var1].iloc[0]\n",
    "    significant_var1 = var1_row['Significant']\n",
    "    adj_p_value_var1 = var1_row['p_adj']\n",
    "    \n",
    "    var2_row = results_df.loc[results_df['Feature'] == var2].iloc[0]\n",
    "    significant_var2 = var2_row['Significant']\n",
    "    adj_p_value_var2 = var2_row['p_adj']\n",
    "    \n",
    "    if significant_var1 != significant_var2:\n",
    "        opposite_significance_pairs.append({\n",
    "            'Variable 1': var1,\n",
    "            'Variable 2': var2,\n",
    "            'Correlation': row['Correlation'],\n",
    "            'Variable 1 Significant': significant_var1,\n",
    "            'Variable 2 Significant': significant_var2,\n",
    "            'Variable 1 Adjusted p_value': adj_p_value_var1,\n",
    "            'Variable 2 Adjusted p_value': adj_p_value_var2\n",
    "        })\n",
    "\n",
    "opposite_significance_df = pd.DataFrame(opposite_significance_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureset = nonsignificant_features['Feature']\n",
    "# featureset = df.drop('examinee', axis=1).columns\n",
    "featureset = significant_features['Feature']\n",
    "\n",
    "with open('subject_ID.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "X_train = df_baseline[featureset]\n",
    "y_train = le.transform(df_baseline['examinee'])\n",
    "\n",
    "X_test = df_anger[featureset]\n",
    "y_test = le.transform(df_anger['examinee'])\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "acc_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "f1_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "print(f\"CV Accuracy: {acc_scores.mean()*100:.2f} ± {acc_scores.std()*100:.2f}\")\n",
    "print(f\"CV F1 Score: {f1_scores.mean()*100:.2f} ± {f1_scores.std()*100:.2f}\")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred) * 100\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "print(f\"Test Accuracy (unseen): {test_acc:.2f}\")\n",
    "print(f\"Test F1 Score (unseen): {test_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subject_ID.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "# Split train/test\n",
    "X_test = df_baseline[featureset]\n",
    "y_test = le.transform(df_baseline['examinee'])\n",
    "\n",
    "X_train = df_anger[featureset]\n",
    "y_test = le.transform(df_anger['examinee'])\n",
    "\n",
    "# Model\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "acc_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "f1_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "print(f\"CV Accuracy: {acc_scores.mean()*100:.2f} ± {acc_scores.std()*100:.2f}\")\n",
    "print(f\"CV F1 Score: {f1_scores.mean()*100:.2f} ± {f1_scores.std()*100:.2f}\")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred) * 100\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "print(f\"Test Accuracy (unseen): {test_acc:.2f}\")\n",
    "print(f\"Test F1 Score (unseen): {test_f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
